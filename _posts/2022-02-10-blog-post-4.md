---
layout: post
title: Blog Post 4
---

## <font color = grey>Purpose
---
In this blog post, we will study a simple version of the *spectral clustering* algorithm for clustering data points. We will first see how the method `K-Means` we studied in PIC16A will fail, followed by when spectral clustering is useful and how it can be implemented. 

## <font color = grey>Method
--- 
#### Step 0: Where K-Means Fails
In PIC 16A, we have already studied *K-Means* for custering data points. However, there are some cases that they do not work. Before anything, let's first import the necessary python packages.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

We then produce a shape from the dataset `make_moon`. Let's first take a look of its scatterplot.


```python
np.random.seed(1234)
n = 200

# save the euclidean coordinates of each point to X
# save the label of each point to y
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7f928c493f10>


![BP4_4_1.png](/images/BP4_4_1.png)
    


As we can see, there are two clusters in our datasets: two crescents. Let's take a look how well K-Means can handle this:


```python
# import the package
from sklearn.cluster import KMeans

# running K-Means model
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7f928c4c5f90>




![BP4_6_1.png](/images/BP4_6_1.png)  
    


As we can see from the graph above, the K-Means method did not do a good job in differentiating the two data clusters, primarily because the K-Means method is designed for circular clusters. Then, we will introduce the *spectral-clustering* algorithm to succesfully cluster the datasets. 

#### Step A: Construct the Similarity Matrix
In the first step, we aim to construct the similarity matrix $\mathbf{A}$, which is a matrix (2d `np.ndarray`) with the following criteria:
- it has shape `(n, n)` (recall that we have defined `n` as the no. of data points);
- its entry `A[i,j]` is equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` otherwise; `epsilon` will be provided at this stage
- the diagonal entries `A[i,i]` are all equal to zero.

Here are the codes for `epsilon` equals `0.4`:


```python
# import a package to compute the distance matrix
from sklearn.metrics import pairwise_distances
A = pairwise_distances(X)

epsilon = 0.4

# assign values to entries based on its distance and epsilon
A = np.where(A**2 <= epsilon**2, 1, 0)

# fill the diagnoal with 0
np.fill_diagonal(A, 0)

A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



#### Step B: Compute Binary Normcut Objective
Now we have constructed the similarity matrix $\mathbf{A}$, which stores information about which points are within the distance of epsilon. Clustering the data into two parts is equivalent to partitioning the row and colomns of the matrix $\mathbf{A}$. 

Before we proceed further, we need to define a few technical terms so we can reframe our task to an optimization problem:
- let $C_0$ and $C_1$ be two clusters. By the example dataset in Step 0, we assume that every data point is in either $C_0$ or $C_1$. We can check the cluster membership for point `i` by checking `y[i]`: if `y[i] = 1`, then point `i` is an element of cluster $C_1$;
- the cut term $\mathbf{cut}(C_0, C_1)$ is the no. of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$, computed by summing up the entries `A[i,j]` for each pair of points `(i,j)` such that `i` and `j` are in different clusters. In other words, if this term is small, then the points in $C_0$ is not very close to those in $C_1$;
- the **degree** of $i$, $d_i = \sum_{j = 1}^n a_{ij}$ is the $i$th row-sum of $\mathbf{A}$;
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$. The *volume* of cluster $C_0$ is a measure of the size of the cluster;

And the *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

And now, in order to find a good partition, we have to minimize the *binary norm cut objective*. By observing the function above, we notice that we have to find $C_0$ and $C_1$ such that the $\mathbf{cut}(C_0, C_1)$ term is small and the $\mathbf{vol}(C_0)$ term is large such that the $\frac{1}{\mathbf{vol}(C_0)}$ term is small. In other words, we need to balance between the fact that there are relatively few points relating $C_0$ and $C_1$ and the two clusters cannot be too small.

In this step, we will first write functions to calculate the *binary norm cut objective* by combining the fuction to calculate the two factors of the equation above.

Firstly, the `cut(A, y)` function will compute the cut term for a given similarity matrix and a cluster profile for each point:


```python
def cut(A, y):
    cut = 0
    
    # write a for-loop to iterate over the matrix
    for i in range(len(A)):
        for j in range(len(A[i])):
            # add if the points are from different clusters
            if (y[i] != y[j]) :
                cut += A[i,j]
    
    # divide by 2 because of double-counting
    return cut/2
```

Compute the cut objective for the true cluster profile `y`:


```python
cut(A, y)
```




    13.0



If we run the cut objective function for a randomly generated cluster profile vector, we can see the result is much larger than the true cluster profile `y`:


```python
testing_vector = np.random.randint(0, 2, size = (n))
cut(A, testing_vector)
```




    1150.0



Secondly, the `vols(A,y)` function computes the volumes of the two clusters and return them as a tuple:


```python
def vols(A, y):
    # calculate the degree of each row 
    degree = np.sum(A, axis = 1)
    
    # calculate vol based on its definition
    v0 = degree[y == 0].sum()
    v1 = degree[y == 1].sum()
    
    # return the tuple 
    return v0, v1
```

Combine the two then we can write `normcut(A, y)` to calculate the normcut objective:


```python
def normcut(A, y):
    # call the vols function to calculate the volume of the two clusters
    v0, v1 = vols(A, y)
    
    # use the definition to compute normcut
    return cut(A, y) * (1/v0 + 1/v1)
```

And the `normcut` for our dataset is:


```python
normcut(A, y)
```




    0.011518412331615225



Which is significantly lower than a random clustering:


```python
normcut(A, testing_vector)
```




    1.0240023597759158



#### Step C: Reformulate Binary Normcut Function
To allow efficiency to minimize the *binary normcut objective*, we introduce the following mathematical trick to transform the problem optimization problem. 

If we define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Then by linear algebra, 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

In this step, we want to write a function to construct the vector $\mathbf{z}$ and verify the equation above. The vector now preserves the cluster information by the "sign information". If it is positive, then it belongs to $C_0$; if it is negative, then it belongs to $C_1$. Also notice that this is a case of constrained optimization: $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones. We are also going to verify this after we finish constructing the vector $\mathbf{z}$.


```python
# a function to construct z
def transform(A, y):
    # prepare values for assignment
    v0 = 1 / (vols(A, y)[0])
    v1 = -1 / (vols(A, y)[1])
    
    # assign the values to z based on its cluster
    z = np.where(y == 0, v0, v1)
    
    # return the vector
    return z
```


```python
# compute the vector
z = transform(A, y)
z
```




    array([-0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497,
            0.00043497,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106, -0.00045106,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497,
           -0.00045106, -0.00045106, -0.00045106,  0.00043497,  0.00043497,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106,  0.00043497,
            0.00043497, -0.00045106, -0.00045106, -0.00045106, -0.00045106,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106,  0.00043497,
           -0.00045106,  0.00043497,  0.00043497,  0.00043497,  0.00043497,
            0.00043497,  0.00043497, -0.00045106, -0.00045106, -0.00045106,
            0.00043497,  0.00043497, -0.00045106, -0.00045106,  0.00043497,
            0.00043497, -0.00045106, -0.00045106, -0.00045106,  0.00043497,
            0.00043497,  0.00043497, -0.00045106,  0.00043497, -0.00045106,
            0.00043497,  0.00043497,  0.00043497,  0.00043497, -0.00045106,
           -0.00045106, -0.00045106, -0.00045106,  0.00043497,  0.00043497,
            0.00043497, -0.00045106,  0.00043497, -0.00045106,  0.00043497,
            0.00043497,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497,
            0.00043497, -0.00045106,  0.00043497,  0.00043497, -0.00045106,
           -0.00045106, -0.00045106, -0.00045106,  0.00043497,  0.00043497,
           -0.00045106, -0.00045106, -0.00045106,  0.00043497, -0.00045106,
           -0.00045106,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
            0.00043497,  0.00043497,  0.00043497, -0.00045106,  0.00043497,
            0.00043497,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106, -0.00045106,
            0.00043497, -0.00045106,  0.00043497, -0.00045106, -0.00045106,
            0.00043497,  0.00043497,  0.00043497,  0.00043497, -0.00045106,
           -0.00045106, -0.00045106, -0.00045106, -0.00045106, -0.00045106,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497, -0.00045106,
            0.00043497,  0.00043497,  0.00043497,  0.00043497,  0.00043497,
            0.00043497,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
            0.00043497, -0.00045106,  0.00043497,  0.00043497,  0.00043497,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497, -0.00045106,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106, -0.00045106,
            0.00043497,  0.00043497,  0.00043497,  0.00043497, -0.00045106,
           -0.00045106,  0.00043497,  0.00043497, -0.00045106, -0.00045106,
           -0.00045106,  0.00043497, -0.00045106, -0.00045106, -0.00045106,
            0.00043497, -0.00045106,  0.00043497, -0.00045106, -0.00045106,
           -0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497])




```python
# Verify the new normcut equation

# Compute the diagnal matrix
D = np.diag(np.sum(A, axis = 1))

# Compute LHS and RHS of the equation
LHS = normcut(A, y)
RHS = z@(D-A)@z / (z@D@z)

np.isclose(LHS, RHS)
```




    True




```python
# Verify the identity
np.isclose((z)@D@(np.ones(n)),0)
```




    True



Now we just need to solve the optimization problem with respect to z to find the best clustering of our datasets.

#### Step D: Solve the Optimization
Following the last step, we will solve the constrained optimization problem with the `minimize` function from the python package `scipy.optimize`. 

First, we will substitute for $\mathbf{z}$ the orthogonal complement of $\mathbf{z}$ relative to $\mathbf{D}\mathbf{1}$. We have supplied the following function to take care of this:


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

# a function to compute the orthogonal complement of z relative to D1
def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Then we can go ahead to solve the optimization problem - to minimize the normcut objective: 
**note that we are not exactly finding the optimized value for the normcut objective but an approximation - the z_min can now take any value while the initial vector can only take either 0 or 1.**


```python
# import the necessary packages
from scipy.optimize import minimize
from scipy.optimize import LinearConstraint

# input the constraint 
cons = ({'type': 'eq', 'fun': lambda x: x@D@(np.ones(n))})

# call the minimize function
z_min = minimize(orth_obj, z, constraints = cons).x

z_min
```




    array([-0.00110408, -0.00103307,  0.00091105,  0.00083473,  0.0009572 ,
            0.00089815,  0.00087459,  0.0008855 , -0.00078051, -0.0006884 ,
           -0.0009415 ,  0.00088082,  0.00023671, -0.00087416, -0.00110519,
           -0.00101219, -0.00085024,  0.00091278,  0.00097046,  0.00093537,
           -0.00092703, -0.0006286 , -0.00111412,  0.00082856,  0.00101513,
           -0.00072573,  0.00088533, -0.00089963, -0.00082502,  0.0008815 ,
            0.00088482, -0.00111139, -0.00099592, -0.00097622, -0.00080585,
           -0.00106932,  0.00101513, -0.00111367, -0.00109359,  0.00089606,
           -0.0009736 ,  0.00087568,  0.00087764,  0.001027  ,  0.00101056,
            0.00085594,  0.00090563, -0.00093104, -0.00110383, -0.00033474,
            0.00087418,  0.00088692, -0.00111412, -0.00088543,  0.00102678,
            0.00099514, -0.00106987, -0.00067181, -0.00111538,  0.00092294,
            0.00102823,  0.00097116, -0.0007738 ,  0.00088732, -0.00085963,
            0.00091396,  0.00102823,  0.00101277,  0.00086933, -0.00111358,
           -0.00108198, -0.0011123 , -0.00111383,  0.00089986,  0.00100988,
            0.0006279 , -0.00071722,  0.00086459, -0.00111359,  0.00089337,
            0.00075461,  0.00096331,  0.00101544, -0.00094754, -0.00086939,
           -0.00108967, -0.00097622,  0.00087907,  0.00087548,  0.00087987,
            0.00088295, -0.0007738 ,  0.00088386,  0.00087418, -0.00067181,
           -0.0006884 , -0.00111367, -0.00108648,  0.00097762,  0.00088692,
           -0.0011136 , -0.00110383, -0.00107289,  0.00062293, -0.00098224,
           -0.00095049,  0.00097365,  0.00081281, -0.00104227, -0.00086584,
            0.00096297,  0.00098683,  0.00087399, -0.00101818,  0.00088196,
            0.00102136,  0.00102296,  0.00087459, -0.00067181, -0.0010033 ,
           -0.00111213,  0.00095004, -0.00080333, -0.00110693, -0.00111309,
            0.00100454, -0.00106475,  0.00072685, -0.00111139, -0.00067181,
            0.00100974,  0.00087459,  0.00092294,  0.00088957, -0.00111161,
           -0.00111345, -0.0008648 , -0.00089715, -0.00090372, -0.00109792,
           -0.0007059 , -0.00110823,  0.00101473,  0.00087227, -0.0010983 ,
            0.00101388,  0.00097284,  0.00098102,  0.00088183,  0.00087383,
            0.00087764,  0.00100974,  0.00100988, -0.00072573, -0.0011067 ,
            0.00102823, -0.00110932,  0.00088692,  0.00101388,  0.00101544,
           -0.00103828, -0.00107699,  0.00090603,  0.00087907, -0.00068758,
           -0.00072573, -0.00095049,  0.00087227,  0.00087459,  0.00086921,
           -0.00111274,  0.00098428, -0.00086584, -0.00093814, -0.00108967,
            0.00090191,  0.00087216,  0.00098102,  0.00082389, -0.00110693,
           -0.00085024,  0.00100833,  0.00097116, -0.0007136 , -0.00078051,
           -0.00076871,  0.00088396, -0.00080585, -0.00106987, -0.00110383,
            0.00090191, -0.00103828,  0.00088396, -0.00110883, -0.00095807,
           -0.00101219, -0.0007424 ,  0.00089255,  0.00077105,  0.00089266])



#### Step E: Visualize the Result

How well did we do? Let us plot the data and test it out!. Remember how `z_min` identifies the cluster that the data point belongs to: if it is positive, it belongs to $C_0$, and if it negative, it belongs to $C_1$.


```python
plt.scatter(X[z_min < 0][:,0], X[z_min < 0][:,1], color = 'black')
plt.scatter(X[z_min >= 0][:,0], X[z_min >= 0][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f928de45ad0>




![BP4_36_1.png](/images/BP4_36_1.png)
    


Indeed, the black points and the red points are clearly representing two clusters (with one exception of a red dot). We can see how this algorithm now works better than the `K-Means`.

#### Step F: Be More Efficient
Although we have achieved our objective in Step E, the method introduced is not practical in real life. Instead, we are more likely to use eigenvalues and eigenvectors to solve the equations everytime we encounter such task. According to the Rayleigh-Ritz Theorem, the vector z we want is actually the eigenvector corresponding to the second-smallest eigenvalue of the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$. And the *Laplacian* matrix is defined as 
$$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$


```python
# find the Laplacian matrix of A
L = np.linalg.inv(D) @ (D - A)

# compute its eigenvalue and eigenvectors 
Lam, U = np.linalg.eig(L)
# sort according to its eigenvalues
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]

# 2nd smallest eigenvalue and corresponding eigenvector
z_eig = U[:,1]

z_eig
```




    array([-0.09389207, -0.07736223,  0.06300698,  0.06253406,  0.07203744,
            0.06844687,  0.05944786,  0.06009116, -0.05131329, -0.03946165,
           -0.06620733,  0.06553515,  0.01103184, -0.05996065, -0.08757109,
           -0.07501904, -0.0572584 ,  0.06303476,  0.07571484,  0.07031429,
           -0.0644712 , -0.03290825, -0.09343953,  0.06278897,  0.07955868,
           -0.04567633,  0.06045876, -0.06151979, -0.05555302,  0.06338615,
            0.06034508, -0.09371443, -0.07336238, -0.07147172, -0.05389553,
           -0.08160528,  0.07955868, -0.09261574, -0.08538788,  0.06813982,
           -0.07110439,  0.06321393,  0.06346373,  0.07993405,  0.07933222,
            0.06328915,  0.06307581, -0.06482837, -0.08710903, -0.01347952,
            0.06233862,  0.06056617, -0.09343953, -0.06090757,  0.0796757 ,
            0.07731147, -0.08189833, -0.03602595, -0.09290969,  0.06294241,
            0.0798901 ,  0.07415366, -0.05089988,  0.06505416, -0.05811411,
            0.06897256,  0.0798901 ,  0.07942743,  0.06238392, -0.09332248,
           -0.08361412, -0.09170838, -0.09355416,  0.0631895 ,  0.0790602 ,
            0.05915303, -0.04468668,  0.06320357, -0.09280613,  0.06786225,
            0.05938268,  0.07278969,  0.07817252, -0.06725567, -0.06656592,
           -0.0848015 , -0.07147172,  0.06180046,  0.06269775,  0.06346513,
            0.05980403, -0.05089988,  0.0668405 ,  0.06233862, -0.03602595,
           -0.03946165, -0.09261574, -0.08445433,  0.07597297,  0.06056617,
           -0.09287649, -0.08710903, -0.08223737,  0.04215372, -0.07189665,
           -0.06759655,  0.07304918,  0.06305594, -0.07848701, -0.05923704,
            0.0725521 ,  0.07682128,  0.0626213 , -0.07571056,  0.06316259,
            0.07962072,  0.08018401,  0.05944786, -0.03602595, -0.07403544,
           -0.09199031,  0.07111864, -0.05346263, -0.09010294, -0.09363897,
            0.07783403, -0.08123258,  0.06128186, -0.09371443, -0.03602595,
            0.07861426,  0.05944786,  0.06294241,  0.0613566 , -0.0913628 ,
           -0.09310399, -0.06571268, -0.06171751, -0.06231661, -0.09411248,
           -0.04240836, -0.0902581 ,  0.07880403,  0.06254634, -0.09405611,
            0.0789515 ,  0.07440044,  0.07648554,  0.06338658,  0.06225021,
            0.06346373,  0.07861426,  0.0790602 , -0.04567633, -0.08845431,
            0.0798901 , -0.09378748,  0.06056617,  0.0789515 ,  0.07817252,
           -0.07800947, -0.08257647,  0.06849123,  0.06180046, -0.0368132 ,
           -0.04567633, -0.06759655,  0.06254634,  0.05944786,  0.05906413,
           -0.09190167,  0.07670821, -0.05923704, -0.06554293, -0.0848015 ,
            0.06314009,  0.06319512,  0.07648554,  0.06307343, -0.09010294,
           -0.0572584 ,  0.07809813,  0.07415366, -0.04338972, -0.05131329,
           -0.0499304 ,  0.06306518, -0.05389553, -0.08189833, -0.08710903,
            0.06314009, -0.07800947,  0.06306518, -0.09054168, -0.06864681,
           -0.07501904, -0.04662749,  0.06443896,  0.06148957,  0.06422265])



Plot the data again, using the sign of `z_eig` as the cluster profile. We can see we have achieved the same result as Step E.


```python
plt.scatter(X[z_eig < 0][:,0], X[z_eig < 0][:,1], color = 'black')
plt.scatter(X[z_eig >= 0][:,0], X[z_eig >= 0][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f928e196910>




![BP4_41_1.png](/images/BP4_41_1.png)
    


> **Observation: by using the Laplacian matrix, we can also achieve a correct clustering of the data, with only a small number of points misclustered (only 1 red dot in this case), and much more efficient than the result in Step E. above.**

#### Step G: Synthesize the Result
In this step, we are going to incorporate all the above discussion into one function `spectral_clustering(X, epsilon)` which takes the input data X and the distance threshold `epsilon` and performs spectral clustering.


```python
def spectral_clustering(X, epsilon):
    '''
    A function to perform spectral clustering by the following steps:
    1. Construct the similarity matrix
    2. Construct the Laplacian matrix
    3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    4. Return labels based on the eigenvector
    @param X: user-specified datasets include the Euclidean coordinates of the points (in the same format as Part A)
    @param epsilon: user-specified double that is the distance threshold
    @return: an array of labels indicating data point is in group 0 or in group 1
    '''
    
    # construct the similarity matrix
    A = np.where((pairwise_distances(X))**2 <= epsilon**2, 1, 0)
    np.fill_diagonal(A, 0)
    
    # construct the Laplacian matrix
    D = np.diag(np.sum(A, axis = 1))
    L = np.linalg.inv(D) @ (D - A)
    
    # compute eigenvector with 2nd smallest eigenvalue
    Lam, U = np.linalg.eig(L)
    ix = Lam.argsort()
    
    # return labels based on sign 
    z = np.where(U[:,ix][:,1] < 0, 0, 1)
    
    # return the array
    return z
```

Now we can experiment with the supplied data from the beginning of the problem again with a new `epsilon` = 0.5:


```python
z_exp = spectral_clustering(X, 0.5)
plt.scatter(X[z_exp == 0][:,0], X[z_exp == 0][:,1], color = 'black')
plt.scatter(X[z_exp == 1][:,0], X[z_exp == 1][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f928f20e750>




![BP4_46_1.png](/images/BP4_46_1.png)
    


> **Observation: we have finally included everything and shown that the spectral clustring works for the crescents! By changing the value of distance threshold, we can tweak our result. Here after we increase it to 0.5, the result is a little bit worse because there are a few more wrong color but overall it is a pretty accurate clustering.**

#### Step H: Experiment with Noise

In this step, we are going to generate different datasets using `make_moons` and examine how the algorithm works if we increase our `noise`. 


```python
n = 1000
```


```python
# we start with the lowest noise
X1, y1 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.03, random_state=None)
z1 = spectral_clustering(X1, 0.4)
plt.scatter(X1[z1 == 0][:,0], X1[z1 == 0][:,1], color = 'black')
plt.scatter(X1[z1 == 1][:,0], X1[z1 == 1][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f928f354810>




![BP4_50_1.png](/images/BP4_50_1.png)
    



```python
# we slightly increase our noise
X2, y2 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.08, random_state=None)
z2 = spectral_clustering(X2, 0.4)
plt.scatter(X2[z2 == 0][:,0], X2[z2 == 0][:,1], color = 'black')
plt.scatter(X2[z2 == 1][:,0], X2[z2 == 1][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f9276e21110>




![BP4_51_1.png](/images/BP4_51_1.png)    
    



```python
# lots of noise
X3, y3 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
z3 = spectral_clustering(X3, 0.4)
plt.scatter(X3[z3 == 0][:,0], X3[z3 == 0][:,1], color = 'black')
plt.scatter(X3[z3 == 1][:,0], X3[z3 == 1][:,1], color = 'red')
```




    <matplotlib.collections.PathCollection at 0x7f9277729a90>




![BP4_52_1.png](/images/BP4_52_1.png)
    


> **Observation: as we increase our noise from the three experiments above, we see that the ability to correctly cluster the datapoints decreases for the spectral clustering algorithm. The algorithm is doing pretty decent at the low noise level like n = 0.03, 0.05 (from the steps above) and 0.08 and still find the two half-moon clusters, but clusters a small portion of the datapoints wrongly when we increase n to n = 0.15.** 

#### Step I: Bull's Eye
In the last step, we are going to apply our algorithm to a new dataset `make_circles`. Let us first take a look at the scatterplot:


```python
n = 2000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7f9277f4f090>




![BP4_55_1.png](/images/BP4_55_1.png)
    


As before, the K-Means will not do a good job here:


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7f9278040450>




![BP4_57_1.png](/images/BP4_57_1.png)
    


let us construct a few plots to find an approximate range for `epsilon` such that the *spectral clustering* algorithm correctly separate the two circles:


```python
fig, axarr = plt.subplots(4, 2, figsize = (20, 10))

for i in range(2,10):
    row = (i-2) // 2
    col = i % 2
    
    epsilon = i/10
    z = spectral_clustering(X, epsilon)
    
    axarr[row, col].scatter(X[z == 0][:,0], X[z == 0][:,1], color = 'black')
    axarr[row, col].scatter(X[z == 1][:,0], X[z == 1][:,1], color = 'orange')
    axarr[row, col].set(title = f"Clustering with epsilon = {epsilon}")
    axarr[row, col].axis("off")
```


![BP4_59_1.png](/images/BP4_59_1.png)
    


> **Observation: with an appropriote `epsilon` value, the spectral clustering algorithm will correct separate the two concentric circles. Here, we can see the value of `epsilon` is roughly in between 0.3 and 0.5 (inclusive).**

---
layout: post
title: Blog Post 3
---

## <font color = grey>Purpose
---
In this blog post, we will interact with the `scrapy` python package to use webscraping to answer the following question:
> What movie or TV shows share the most no. of actors with your favorite movie or show?

We will be using the movie [Harry Potter and the Goblet of Fire](https://www.imdb.com/title/tt0330373/) as a starting point. Here is a link to the [repository holding files](https://github.com/SamuelShi6/BlogPost3)(https://github.com/SamuelShi6/BlogPost3). 

## <font color = grey>Method
---
#### Step 0. Setting Up the Project
In this activity, we will mainly interact with the `scrapy` package to access online information and navigate through webpages. 
In order to start a project, we first run the following code in the local terminal:
```
conda activate PIC16B
scrapy startproject IMDB_scraper
cd IMDB_scraper
```
Then we proceed to the `spiders` folder to create a file named `imdb_spider.py`. 
As always, we first have to import the necessary package:
```python
import scrapy
```
In order to interact with scrapy, we usually write a class that inherit from the built-in `Spider` class. Then we write a name so it can be called to execute our webscaping methods. We have to include an url to start from - in this case, we include the IMDB link for my favorite movie: Harry Potter and the Goblet of Fire. 
```python
# start by writing a new class inherited from Spider
class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'

    # url for my favorite movie
    start_urls = ['https://www.imdb.com/title/tt0330373/']
```

#### Step 1. Writing the Parsing Methods
This is the backbone of this activity. We will be writing three parsing methods inside the `ImdbSpider` class. Basically, the each parsing method will address the following task in order:
- locate the cast & crew page
- locate the actor's page
- locate the films that the actors have worked on 

Here is the detailed breakdown of each task:
-  `parse(self, response)` assumes we start from the home page of the movie, and helps us navigate to the *Cast & Crew* Page. By experimenting with the IMDB webpage, we realize that the *Cast & Crew* Page always adds a `fullcredits/` to the homepage of the movie. Therefore, we can simply append that to the start_url. In the end, we will call the method `parse_full_credits(self,response)` by specifying this method in the `callback` argument to a yielded `scrapy.Request`. Basically, we need to run the `callback` method to the specified url. 

```python
def parse(self, response):
    '''
    assumes we start from the home page of the movie, and helps us navigate to the Cast & Crew Page.
    In the end, we will call the method `parse_full_credits(self,response)`
    @param response: an object that helps us crawl the web
    @return : this method does not return any data
    '''

    # specify the Cast & Crew Page
    navigation_link = response.url + 'fullcredits/'

    # call the parse_full_credits() method to the Cast & Crew page
    yield scrapy.Request(navigation_link, callback = self.parse_full_credits)
```

- `parse_full_credits(self, response)` assume that we start on the *Cast & Crew* page and yields a `scrapy.Request` for the page of each actor listed on the page. By turning on the developer's tool on the browser and hovering on the actors' profile photos, we can locate the necessary links for each of the actors. For each actor, we then call the method `parse_actor_page(self, response)`. Note that `response` is a very useful object of the class that can help use identify and find the links/contents that we need to store; for instance, what class the link to the actor's page belongs to and how to get the link. Usually, we can run `scrapy shell ` and the url to create a virtual envrionment in the terminal to interact with `response` before coding in the actual file. 

```python
def parse_full_credits(self, response):
    '''
    assume that we start on the Cast & Crew page and yields a scrapy.Request for the page of each actor listed on the page
    @param response: an object that helps us crawl the web
    @return : this method does not return any data
    '''
    # use list comprehension to generate the actor's list
    actor_list = [a.attrib["href"] for a in response.css("td.primary_photo a")]

    # write a for-loop to call the next method on each actor's page
    for actor in actor_list:
        yield scrapy.Request('https://www.imdb.com'+actor, callback = self.parse_actor_page)
```
- `parse_actor_page(self, response)` assume that we are on the page of an actor and yields a dictionary with two key-value pairs, of the form {"actor" : actor_name, "movie_or_TV_name" : movie_or_TV_name}. Note that the method should yield one such dictionary for each of the movies or TV shows on which that actor has worked. Notice that there are some "episodes" embedded in the list of productions, but we are only interested in those that have valid imdb pages because we here assume that a valid imdb page corresponds to a "real" nonoverlapping production by the actor. Use similar methods of developer's tool and `scrapy shell` to retrieve the necessary text (actor's name and the name of the work that the actor has worked on). 

```python
def parse_actor_page(self, response):
    '''
    assume that we are on the page of an actor and yields a dictionary with two key-value pairs, 
    of the form {"actor" : actor_name, "movie_or_TV_name" : movie_or_TV_name}. 
    the method should yield one such dictionary for each of the movies or TV shows on which that actor has worked.
    @param response: an object that helps us crawl the web
    '''
    # find the actor's name
    actor_name = response.css("span.itemprop::text").get()

    # find the list of the work that the actor has worked on 
    rough_film_list = response.css("div.filmo-row")
    # we are only interested in those with a valid IMDB page
    exact_list = [i.css("a::text").get() for i in rough_film_list if i.css("::attr(id)") and i.css("a")]
    
    # yield a dictionary for each of the film 
    for film in exact_list:
        yield {
            "actor": actor_name,
            "movie_or_TV_name": film
        }
```

Run the following command line in the terminal and all results will be exported to the file `movies.csv`.
```
scrapy crawl imdb_spider -o movies.csv
```

#### Step 2. Intepretating a Result
After we have stored the information we need to the csv file, we can employ the powerful pandas to finally derive an answer for our question. 
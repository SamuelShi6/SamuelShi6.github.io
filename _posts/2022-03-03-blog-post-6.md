## <font color = grey>Purpose
---
In this blog post, we will use Tensorflow tools such as text classification and mixed data features (specifically, the functional keras API) to develop and assess a fake news classifier. Our data can be found it here from [kaggle.](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
<br><br>



## <font color = grey>Method
--- 
#### Step I. Acquire Training Data
As always, we need to import the necessary packages before everything. We may return back to this part and add more packages later on as we work on this project.


```python
# tensorflow packages and related API 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# ploting packages
from matplotlib import pyplot as plt
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"

# other relevant packages
import pandas as pd
from sklearn.feature_extraction import text
import numpy as np
import re
import string
```

The target training data is hosted at the url specified by `train_url` below. We can read it into a dataframe `df` directly:


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
```

Let us take a close look of the table. The `title` column gives the title of the article; the `text` column gives the full article text; the final `fake` column identifies if the news is fake or not - it has a value of `0` if it is true and `1` otherwise. 


```python
df.head()
```





  <div id="df-d045034b-67a4-4146-9012-6ba073cd0298">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-d045034b-67a4-4146-9012-6ba073cd0298')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-d045034b-67a4-4146-9012-6ba073cd0298 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-d045034b-67a4-4146-9012-6ba073cd0298');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




#### Step II. Make a Dataset
Before working on the model, we want to first write a function `make_dataset` to produce a `Dataset. It will take the dataframe as an input and produce a dataset as an output with the following modifications:
- it removes stopwords like `and`, `the`, and `but` etc.
- the dataset should have two inputs `(title, text)` and one output containing the `fake` column. 



```python
# create a set to remove punctuations
punctuation = [ c for c in string.punctuation ] + [u'\u201c',u'\u201d',u'\u2018',u'\u2019']
punctuation = set(punctuation)

# create a set to remove stopwords
stop = text.ENGLISH_STOP_WORDS

# write a helper function to process the strings
def preprocessing(text0):
  # change all char to lowercase 
  text1 = text0.lower()

  # remove all punctuations
  text2 = ''.join([ch for ch in text1 if ch not in (punctuation)])

  # remove stopwords
  text3 = ' '.join([word for word in text2.split() if word not in (stop)])

  # return the 'cleaned' string
  return text3

# a function to generate dataset
def make_dataset(df):
  # process the two columns
  df['title'] = df['title'].apply(preprocessing)
  df['text'] = df['text'].apply(preprocessing)

  # create a dataset with two inputs and one output
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
  )

  # batch the dateset to increase training speed
  return data.batch(100)
```

Now run the function on our training dataframe to produce a `Dataset`:


```python
data = make_dataset(df)
```

Split 20% of the data for validation:


```python
# specify training size and validation size
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

# take the data from the dataset 
train = data.take(train_size)
val   = data.skip(train_size).take(val_size)
```

Then we want to see what the base rate is, meaning what benchmark our model should hit to produce valuable data science work:


```python
# first create an iterator
labels_iterator= train.unbatch().map(lambda texts, fake: fake['fake']).as_numpy_iterator()
labels_iterator

# iterate the labels to count the no. 
no_fake = 0
no_true = 0
for labels in labels_iterator:
  if labels == 0:
     no_true += 1
  else:
     no_fake += 1

# print the % of fake news in our dataset
print("There are " + str(no_true) + " pieces of true news.")
print("There are " + str(no_fake) + " pieces of fake news.")
print("The base rate for detecting fake news is " + "{:.2%}".format(no_fake/(no_true+no_fake)) +".")
```

    There are 8603 pieces of true news.
    There are 9397 pieces of fake news.
    The base rate for detecting fake news is 52.21%.


Now, the accuracy of our model has to be at least higher than **52.21%.** Otherwise, it won't even beat the probability of randomly guessing the validity of the traget news. 
<br><br>

#### Step III. Create Models

**Model 1: Only Using Title as Input**

*Vectorization* refers to the process of representing text as a vector. We need to adapt the "vectorization" to our texts since our models can only compute numbers. Usually, we first standardize our text to remove capitals, punctuations, and HTML or other non-semantic content to make it less "messy". Then we can construct a "vectorization" layer to represent them in vectors that can be "understood" by our the layers. 


```python
# no. of words to be tracked
size_vocabulary = 2000

# standardization
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

# create a vectorizaion layer for title column
vectorize_layer1 = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

# adapt the layer
vectorize_layer1.adapt(train.map(lambda x, y: x["title"]))
```

We need to specify the two kinds of `keras.Input` for our model. The following are the necessary information we need to include for them:
- `shape` is the shape of a single item of data
- `name` is the descriptive name that helps us to remember for later
- `dtype` is the type of the data contrained in the input tensor


```python
# inputs
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

For model one, we just need to use the first `title_input`. 

Now create some layers for our model. There is one interesting `Embedding` layer: the word means the presentation of a word in a vector space, aiming to create a representation such that words with related "meanings" are close to each other in a vector space, We choose the output dimension to be 5, meaning we aim to represent these vectors in a 5 dimensional space. 


```python
# layers for processing the title
title_features = vectorize_layer1(title_input)
title_features = layers.Embedding(size_vocabulary, 5, name = "embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```

Time to construct our model 1. Our output should have the number of ouputs to be 2 because there are 2 categories: true and false in our dataset. Let's take a look at its summary:


```python
model1 = keras.Model(
    inputs = [title_input],
    outputs = layers.Dense(2, name = "fake")(title_features)
)

model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding1 (Embedding)      (None, 500, 5)            10000     
                                                                     
     dropout (Dropout)           (None, 500, 5)            0         
                                                                     
     global_average_pooling1d (G  (None, 5)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 5)                 0         
                                                                     
     dense (Dense)               (None, 32)                192       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 10,258
    Trainable params: 10,258
    Non-trainable params: 0
    _________________________________________________________________


Train our model as follows:


```python
# specify loss function
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# train the model
history1 = model1.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 10ms/step - loss: 0.6924 - accuracy: 0.5156 - val_loss: 0.6916 - val_accuracy: 0.5266
    Epoch 2/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.6909 - accuracy: 0.5277 - val_loss: 0.6876 - val_accuracy: 0.5266
    Epoch 3/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.6771 - accuracy: 0.5937 - val_loss: 0.6566 - val_accuracy: 0.5586
    Epoch 4/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.6045 - accuracy: 0.7697 - val_loss: 0.5337 - val_accuracy: 0.8442
    Epoch 5/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.4650 - accuracy: 0.8502 - val_loss: 0.3994 - val_accuracy: 0.8667
    Epoch 6/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.3583 - accuracy: 0.8797 - val_loss: 0.3172 - val_accuracy: 0.8885
    Epoch 7/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.2969 - accuracy: 0.8958 - val_loss: 0.2675 - val_accuracy: 0.9013
    Epoch 8/20
    180/180 [==============================] - 2s 11ms/step - loss: 0.2564 - accuracy: 0.9081 - val_loss: 0.2363 - val_accuracy: 0.9099
    Epoch 9/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.2309 - accuracy: 0.9154 - val_loss: 0.2163 - val_accuracy: 0.9146
    Epoch 10/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.2100 - accuracy: 0.9219 - val_loss: 0.1986 - val_accuracy: 0.9234
    Epoch 11/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1980 - accuracy: 0.9261 - val_loss: 0.1878 - val_accuracy: 0.9245
    Epoch 12/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1861 - accuracy: 0.9311 - val_loss: 0.1816 - val_accuracy: 0.9247
    Epoch 13/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1749 - accuracy: 0.9346 - val_loss: 0.1757 - val_accuracy: 0.9261
    Epoch 14/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1684 - accuracy: 0.9371 - val_loss: 0.1711 - val_accuracy: 0.9263
    Epoch 15/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1642 - accuracy: 0.9365 - val_loss: 0.1687 - val_accuracy: 0.9261
    Epoch 16/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1575 - accuracy: 0.9403 - val_loss: 0.1650 - val_accuracy: 0.9294
    Epoch 17/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1537 - accuracy: 0.9423 - val_loss: 0.1631 - val_accuracy: 0.9294
    Epoch 18/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1493 - accuracy: 0.9435 - val_loss: 0.1653 - val_accuracy: 0.9267
    Epoch 19/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1482 - accuracy: 0.9453 - val_loss: 0.1601 - val_accuracy: 0.9317
    Epoch 20/20
    180/180 [==============================] - 2s 9ms/step - loss: 0.1421 - accuracy: 0.9459 - val_loss: 0.1595 - val_accuracy: 0.9321


Plot the performances as follows:


```python
plt.plot(history1.history["accuracy"])
plt.plot(history1.history["val_accuracy"])
```




    [<matplotlib.lines.Line2D at 0x7f17f02a5c90>]




![Blog_Post_6_26_1.png](/images/Blog_Post_6_26_1.png)
    


> **Observation:** Our model 1 starts at around 53%, which is close to the base rate, jumps to over 90% after a few iterations and finally stablizes around 93% for the validation accuracy. No overfitting observed for model 1 as training accuracy and validation accuracy are very close. 

<br>

**Model 2: Only Using Text as Input**

The idea and procedure of our second model is pretty much the same as our first one. We first create a second vectorization layer, add an embedding layer, and add some more layers:


```python
size_vocabulary = 2000

# create a vectorization layer
vectorize_layer2 = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer2.adapt(train.map(lambda x, y: x["text"]))

# layers for processing the lyrics, pretty much the same as from our lecture
# on text classification
text_features = vectorize_layer2(text_input)
text_features = layers.Embedding(size_vocabulary, 5, name = "embedding2")(text_features)
text_features = layers.Dropout(0.3)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.3)(text_features)
text_features = layers.Dense(64, activation='relu')(text_features)
```

First we take a look at the model summary:


```python
model2 = keras.Model(
    inputs = [text_input],
    outputs = layers.Dense(2, name = "fake")(text_features)
)

model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization_1 (TextV  (None, 500)              0         
     ectorization)                                                   
                                                                     
     embedding2 (Embedding)      (None, 500, 5)            10000     
                                                                     
     dropout_2 (Dropout)         (None, 500, 5)            0         
                                                                     
     global_average_pooling1d_1   (None, 5)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 5)                 0         
                                                                     
     dense_1 (Dense)             (None, 64)                384       
                                                                     
     fake (Dense)                (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 10,514
    Trainable params: 10,514
    Non-trainable params: 0
    _________________________________________________________________


Train the model as follows:


```python
# specify loss function
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# train the model
history2 = model2.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 15ms/step - loss: 0.6619 - accuracy: 0.6375 - val_loss: 0.5534 - val_accuracy: 0.9222
    Epoch 2/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.3646 - accuracy: 0.9247 - val_loss: 0.2195 - val_accuracy: 0.9600
    Epoch 3/20
    180/180 [==============================] - 2s 14ms/step - loss: 0.1887 - accuracy: 0.9553 - val_loss: 0.1410 - val_accuracy: 0.9703
    Epoch 4/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1396 - accuracy: 0.9674 - val_loss: 0.1107 - val_accuracy: 0.9751
    Epoch 5/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1149 - accuracy: 0.9723 - val_loss: 0.0943 - val_accuracy: 0.9777
    Epoch 6/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.1001 - accuracy: 0.9748 - val_loss: 0.0849 - val_accuracy: 0.9811
    Epoch 7/20
    180/180 [==============================] - 3s 14ms/step - loss: 0.0885 - accuracy: 0.9784 - val_loss: 0.0778 - val_accuracy: 0.9820
    Epoch 8/20
    180/180 [==============================] - 2s 14ms/step - loss: 0.0803 - accuracy: 0.9795 - val_loss: 0.0726 - val_accuracy: 0.9831
    Epoch 9/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0726 - accuracy: 0.9812 - val_loss: 0.0689 - val_accuracy: 0.9845
    Epoch 10/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0696 - accuracy: 0.9822 - val_loss: 0.0669 - val_accuracy: 0.9825
    Epoch 11/20
    180/180 [==============================] - 2s 14ms/step - loss: 0.0651 - accuracy: 0.9826 - val_loss: 0.0653 - val_accuracy: 0.9847
    Epoch 12/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0585 - accuracy: 0.9845 - val_loss: 0.0626 - val_accuracy: 0.9845
    Epoch 13/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0561 - accuracy: 0.9854 - val_loss: 0.0613 - val_accuracy: 0.9845
    Epoch 14/20
    180/180 [==============================] - 2s 14ms/step - loss: 0.0503 - accuracy: 0.9868 - val_loss: 0.0605 - val_accuracy: 0.9845
    Epoch 15/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0486 - accuracy: 0.9863 - val_loss: 0.0603 - val_accuracy: 0.9845
    Epoch 16/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0468 - accuracy: 0.9865 - val_loss: 0.0607 - val_accuracy: 0.9834
    Epoch 17/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0445 - accuracy: 0.9879 - val_loss: 0.0582 - val_accuracy: 0.9845
    Epoch 18/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0420 - accuracy: 0.9890 - val_loss: 0.0589 - val_accuracy: 0.9829
    Epoch 19/20
    180/180 [==============================] - 2s 13ms/step - loss: 0.0406 - accuracy: 0.9900 - val_loss: 0.0586 - val_accuracy: 0.9856
    Epoch 20/20
    180/180 [==============================] - 2s 14ms/step - loss: 0.0378 - accuracy: 0.9899 - val_loss: 0.0596 - val_accuracy: 0.9834


Plot performances as follows:


```python
plt.plot(history2.history["accuracy"])
plt.plot(history2.history["val_accuracy"])
```




    [<matplotlib.lines.Line2D at 0x7f17f001f310>]




![Blog_Post_6_34_1.png](/images/Blog_Post_6_34_1.png)
    


> **Observation**: the validation accuracy quickly increases to 98% and is consistently higher than 98%. This is significantly better than our result in Model 1. No overfitting observed as well since training accuracy and validation accuracy are very close. 

<br>

**Model 3: Include Both Title and Text**

In our last model, we are going to include both the titles and from the texts to train our model. Remember that we have created some layers to each in the first two models. To combine both features, we first need to `concatenate` the ouput of the `title` pipeline with that of the `text` pipleline as follows:


```python
main = layers.concatenate([title_features, text_features], axis = 1)

```

And we can pass the consoildated set of computed features through a few more `Dense` layers. But bear in mind that the last ouput layer must have the number of outputs equal to 2, because the news is either true of false in our dataset. 


```python
main = layers.Dense(32, activation='relu')(main)
```

Construct the model as follows:


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = layers.Dense(2, name = "fake")(main)
)

model3.summary()
```

    Model: "model_3"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  
     ization)                                                                                         
                                                                                                      
     text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   
     orization)                                                                                       
                                                                                                      
     embedding1 (Embedding)         (None, 500, 5)       10000       ['text_vectorization[0][0]']     
                                                                                                      
     embedding2 (Embedding)         (None, 500, 5)       10000       ['text_vectorization_1[0][0]']   
                                                                                                      
     dropout (Dropout)              (None, 500, 5)       0           ['embedding1[0][0]']             
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 5)       0           ['embedding2[0][0]']             
                                                                                                      
     global_average_pooling1d (Glob  (None, 5)           0           ['dropout[0][0]']                
     alAveragePooling1D)                                                                              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 5)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_1 (Dropout)            (None, 5)            0           ['global_average_pooling1d[0][0]'
                                                                     ]                                
                                                                                                      
     dropout_3 (Dropout)            (None, 5)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dense (Dense)                  (None, 32)           192         ['dropout_1[0][0]']              
                                                                                                      
     dense_1 (Dense)                (None, 64)           384         ['dropout_3[0][0]']              
                                                                                                      
     concatenate_1 (Concatenate)    (None, 96)           0           ['dense[0][0]',                  
                                                                      'dense_1[0][0]']                
                                                                                                      
     dense_3 (Dense)                (None, 32)           3104        ['concatenate_1[0][0]']          
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_3[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 23,746
    Trainable params: 23,746
    Non-trainable params: 0
    __________________________________________________________________________________________________


Train the model as follows:


```python
# specify loss function
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# train the model
history3 = model3.fit(train, 
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20
    180/180 [==============================] - 5s 19ms/step - loss: 0.1278 - accuracy: 0.9861 - val_loss: 0.0325 - val_accuracy: 0.9915
    Epoch 2/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.0253 - val_accuracy: 0.9930
    Epoch 3/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0113 - accuracy: 0.9974 - val_loss: 0.0232 - val_accuracy: 0.9930
    Epoch 4/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.0239 - val_accuracy: 0.9930
    Epoch 5/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.0239 - val_accuracy: 0.9930
    Epoch 6/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.0254 - val_accuracy: 0.9935
    Epoch 7/20
    180/180 [==============================] - 3s 18ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0255 - val_accuracy: 0.9928
    Epoch 8/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0240 - val_accuracy: 0.9928
    Epoch 9/20
    180/180 [==============================] - 3s 18ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0299 - val_accuracy: 0.9933
    Epoch 10/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0261 - val_accuracy: 0.9935
    Epoch 11/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0258 - val_accuracy: 0.9933
    Epoch 12/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0275 - val_accuracy: 0.9928
    Epoch 13/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0317 - val_accuracy: 0.9924
    Epoch 14/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0300 - val_accuracy: 0.9928
    180/180 [==============================] - 3s 17ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9926
    Epoch 16/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0302 - val_accuracy: 0.9926
    Epoch 17/20
    180/180 [==============================] - 3s 19ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0334 - val_accuracy: 0.9937
    Epoch 18/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 0.0323 - val_accuracy: 0.9928
    Epoch 19/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0315 - val_accuracy: 0.9937
    Epoch 20/20
    180/180 [==============================] - 3s 17ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0304 - val_accuracy: 0.9933


Let us take a look at the performances:


```python
plt.plot(history3.history["accuracy"])
plt.plot(history3.history["val_accuracy"])
```




    [<matplotlib.lines.Line2D at 0x7f17db0a1950>]




![Blog_Post_6_44_1.png](/images/Blog_Post_6_44_1.png)  
    


> **Observation:** The validation accuracy is significaly higher than model 1 and model 2: it delivers results consistently higher than 99.2% and stablizes around 99.3%. Although it looks like the gap between validation accuracy and training accuracy is relatively large in the graph, their differences are relatively small in terms of an absolute scale. Therefore, there is not overfitting for our model 3. As a result, **we recommend use both the title and text as input for the algorithms when seeking to detect fake news.**

<br>

#### Step IV. Model Evaluation
We can use our `model3` to test how well our model works for another dataset:


```python
# download the dataframe
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_data = pd.read_csv(test_url)

# create the dataset
test_data = make_dataset(test_data)
```

Evaluate the accuracy as follows:


```python
model3.evaluate(test_data)
```

    225/225 [==============================] - 2s 9ms/step - loss: 0.0450 - accuracy: 0.9902





    [0.04496835172176361, 0.9902445673942566]



Great! Our model has achieved an overall accuracy of over **99%!** It turns out our model is very effective in detecting the fake news. 

#### Step V. Embedding Visualization
In our last step, we want to visualize on the embedding that our model has learned. We can take a look at our `embedding1` from model 3. We will reduce the dimention to 2 to visualize the result as follows:


```python
# get the weights from the embedding layer
weights = model3.get_layer('embedding1').get_weights()[0] 

# get the vocabulary from our data prep for later
vocab = vectorize_layer1.get_vocabulary()                

# reduce the data to a 2d representation
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

# create a dataframe
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

# make the plot
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 10,
                 hover_name = "word")

fig.show()
```
{% include embedding.html %}


> **Observation:** it seems that our graph is "streched out" along the horizontal axis - along 2 directions, with one direction corresponding to each of the 2 categories (true and false). On the left, we can find  clustered words such as "ukraine", "chinas", "myanmar", "zimbabwe", "japan", etc. They indeed share similar meanings as they represent countries and international politics. They probably are written more in a serious manner and therefore, more indicative of "truthness" in the news. On the right, we can find words such as "lol", "awesome", "whoa", etc. which are usually informal, and probably they are more indicative of a potential fake news source. 


```python
from plotly.io import write_html
write_html(fig, "embedding.html")
```
